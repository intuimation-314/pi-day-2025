\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}

\begin{document}

\title{The Evolution of $\pi$: Past, Present and Future}
\author{Your Name}
\date{March 14, 2025}
\maketitle

\section*{Introduction}

$\pi$ is one of the most fundamental mathematical constants we encounter early in our life. It comes from Circles which are everywhere—in nature, in the wheels we use, in the ripples on water, in the orbits of planets, in the very structure of galaxies. It is woven into the fabric of the universe—perhaps the first universal constant we intuitively discover.

At its core, $\pi$ is simply the \textbf{ratio of a circle’s circumference to its diameter}:

\[
\pi = \frac{C}{d} \approx 3.14
\]

This means that if you \textbf{unfold the perimeter} of a perfect circle, it would fit \textbf{three diameters exactly}, with a \textbf{tiny bit left over}.  

But here's is the catch - that’s not how we actually compute $\pi$. Measuring a circle directly, no matter how \textbf{precise our tools}, always leads to \textbf{errors}. Engineers might use $\pi \approx 3.14$ or even just $3$, and to the naked eye, \textbf{a circle would still look perfect}.  

But mathematicians? \textbf{They aren’t satisfied.} They want infinite precision.

For centuries, the quest for π’s accuracy has driven both mathematics and computing forward, leading to formulas that look almost unnatural—some even downright terrifying in their complexity!

Even with the fastest algorithm we have today (the Chudnovsky algorithm), in 2022, Google managed to compute π to over 100 trillion digits. And guess what? That required:

\begin{itemize}
    \item \textbf{157 days of computation}.
    \item \textbf{82,000 terabytes of storage}.
    \item Over \textbf{\$200,000 in electricity costs}.
\end{itemize}

At this point, you might be wondering—why? Why does π get so ridiculously complicated? And why do we even need to compute more and more digits?

Well, computing π isn’t just about math for the sake of math—there’s a much bigger reason behind it. And that’s exactly what we’ll explore in this video. 

To understand why π is so important—and why we keep chasing more digits—we need to look back in time. π wasn’t always known with such precision. The journey of π’s discovery spans thousands of years, and we can divide its history into three major eras:

\begin{enumerate}
    \item \textbf{The Geometric Era (250 BCE – 1630 CE)}: When $\pi$ was calculated using \textbf{polygons}, requiring \textbf{millions of sides} just to reach a few decimal places.  
    \item \textbf{The Infinite Series Era (1600s – 1980s)}: When $\pi$ computations \textbf{accelerated dramatically} using calculus and infinite series.  
    \item \textbf{The Modern Algorithm Era (1980 – Present)}: When $\pi$'s precision skyrocketed to \textbf{trillions of digits}, thanks to \textbf{high-speed algorithms and supercomputers}.  
\end{enumerate}

\section*{Era 1: The Geometric Era (250 BCE – 1630 CE)}

The first true breakthrough in computing $\pi$ came from \textbf{Archimedes} around \textbf{250 BCE}. Before Archimedes, civilizations such as the \textbf{Babylonians and Egyptians} approximated $\pi$ through direct measurements. However, these methods were experimental, and they lacked a rigorous mathematical foundation.  

Archimedes took a completely different approach. Instead of measuring circles, he trapped a circle between two polygons—one inside (inscribed) and one outside (circumscribed).  

He started with a hexagon (6-sided polygon) and doubled the number of sides, making the shape increasingly circular:

\[
6 \to 12 \to 24 \to 48 \to 96
\]

By the time he reached a 96-sided polygon, he calculated the bounds for π:

\[
\frac{223}{71} < \pi < \frac{22}{7}
\]

or numerically:

\[
3.1408 < \pi < 3.1429
\]

This was the most accurate estimate of $\pi$ for over \textbf{1,500 years}.  Even today, the fraction 22/7 (used in schools as an approximation for π) comes from Archimedes’ work. 

Even by \textbf{1630}, after centuries of refinement, the best polygon-based calculations could only determine $\pi$ to \textbf{39 decimal places}. And just to achieve that, polygon methods required \textbf{millions of sides}. This was inefficient, and mathematicians needed a new approach.

\section*{Phase 2: The Infinite Series Era (1600s – 1980s)}

In the \textbf{17th century}, the discovery of \textbf{calculus} transformed how $\pi$ was computed. Mathematicians replaced geometric approximations with \textbf{infinite summations}.  

Newton computed $\pi$ without polygons for the first time! \textbf{Isaac Newton} represented $\pi$ using an integral of the quarter-circle:

\[
\frac{\pi}{12} + \frac{\sqrt{3}}{8} = \int_0^{1/2} \sqrt{1 - x^2} \, dx
\]

Expanding the function as a binomial series:

\[
\sqrt{1 - x^2} = 1 - \frac{x^2}{2} - \frac{x^4}{8} - \dots
\]
This was a game changer.

For a deeper dive, \textbf{Veritasium has a fantastic video on this topic—check it out!}

But the quest for $\pi$ continued. The breakthrough came in \textbf{1706}, when \textbf{John Machin}, inspired by the arctan series, introduced an algorithm that converged much faster:

\[
\frac{\pi}{4} = 4 \tan^{-1} \frac{1}{5} - \tan^{-1} \frac{1}{239}
\]

This converged \textbf{100 times faster} than the basic arctan series and newtons formula. Using it, Machin calculated $\pi$ to \textbf{100 decimal places} by hand.

For over two centuries, mathematicians followed in Machin’s footsteps, manually pushing π further, digit by digit.

By the \textbf{20th century}, mechanical computers were already invented rapidly accelerated $\pi$ calculations, We no longer needed computaions by hand.

The first major leap happened in 1949, when the ENIAC computer calculated 2,037 digits of π in just 70 hours.

As computing power grew, so did π records:
- 1973: Computers pushed π past 1 million digits.
- 1980s: Mathematicians realized older formulas were too slow for modern computers.

A new era of π computation had begun.

\section*{The Modern Algorithm Era: The 1980s Revolution in $\pi$ Computation}

We are currently in the third era of $\pi$ calculation. It began around  1980 when mathematicians discovered how to utilise a combination of three independent developments.

First, the \textbf{Fast Fourier Transform (FFT)} algorithm significantly sped up multiplication—an essential operation in all $\pi$ computations. With FFT, multiplying long numbers became nearly linear in time, drastically reducing computation time.

Second major breakthrough was the development of, new high-performance algorithms, specifically designed for $\pi$.

\textbf{Srinivasa Ramanujan} discovered miraculous formula which converged far faster than previous methods like Machin's formula:

\[
\frac{1}{\pi} = \frac{2\sqrt{2}}{9801} \sum_{n=0}^{\infty} \frac{(4n)! (1103 + 26390n)}{(n!)^4 396^{4n}}
\]

Each term of this series added \textbf{8 decimal places}!

Despite the astonishing accuracy of his formula, Ramanujan never provided a formal proof for it—yet decades later, mathematicians confirmed he was right all along.

Later in 1989, The Chudnovsky brothers (David and Gregory Chudnovsky) modified Ramanujan’s method and optimized the series even further:

\[
\frac{1}{\pi} = 12 \sum_{n=0}^{\infty} \frac{(-1)^n (6n)! (13591409 + 545140134n)}{(3n)! (n!)^3 640320^{3n + 3/2}}
\]

Each term in this formula adds \textbf{15 digits}—twice as fast as Ramanujan's

Third, supercomputer advances made it possible to explode $\pi$ to billions, then trillions of digits.

1989: The Chudnovsky brothers calculated 1 billion digits—a world record at the time.
1999: π was computed to 68 billion digits.
2019: Google Cloud reached 31.4 trillion digits.
2022: The record hit 100 trillion digits!

\section*{The Future of $\pi$ Computation}

Today, $\pi$ has been computed to \textbf{over 100 trillion digits}. But how far can we go? For decades, the goal was to compute $\pi$ from the beginning to as many digits as possible. But since the 1990s, researchers have shifted their focus to a new challenge: \textbf{computing individual digits at extreme positions without needing all previous ones}.  

A major breakthrough came in \textbf{1995} with the discovery of the \textbf{BBP Algorithm (Bailey-Borwein-Plouffe)}:

\[
\pi = \sum_{n=0}^{\infty} \frac{1}{16^n} \left( \frac{4}{8n+1} - \frac{2}{8n+4} - \frac{1}{8n+5} - \frac{1}{8n+6} \right)
\]

For the first time, this formula allowed researchers to compute the hexadecimal digits \textbf{ specific (base-16) of $\pi$ without calculating all previous digits}. However, this method only works for hexadecimal and binary digits—\textbf{we still don’t have an efficient way to compute individual decimal digits of $\pi$}.  

\section*{Why Compute More Digits of $\pi$?}

For most real-world applications, we don’t actually need trillions of digits of $\pi$. In fact, just \textbf{10 digits} are enough for engineering and scientific calculations. \textbf{39 digits} are enough to calculate the \textbf{volume of the observable universe} to atomic precision. Even high-precision physics simulations, space exploration, and quantum mechanics rarely require more than a \textbf{few hundred digits}.  

And yet, we continue computing $\pi$ to \textbf{trillions of digits}. So, why do we do it?  

\subsection*{1. Testing Computers and Algorithms}

One of the biggest reasons is \textbf{testing computers and algorithms}. Computing $\pi$ requires an enormous number of calculations—\textbf{trillions of arithmetic operations}—which makes it the \textbf{perfect stress test for modern processors}.  

In fact, companies like \textbf{Intel and AMD} have used $\pi$ calculations to \textbf{detect hidden errors in processors} before releasing them to the public. If a CPU or supercomputer can compute $\pi$ \textbf{correctly for trillions of digits}, it means the hardware is \textbf{stable and reliable}.  

\subsection*{2. The Unsolved Mysteries of $\pi$}

But it’s not just about testing hardware—Mathematicians are also searching for \textbf{patterns in $\pi$'s digits}. Despite computing \textbf{trillions of digits}, no repeating pattern has ever been found. But could there be a hidden structure? Could $\pi$ be linked to \textbf{deep, undiscovered properties in number theory}?  

By computing more digits, researchers can analyze $\pi$ statistically and \textbf{search for clues to its nature}.

\subsection*{3. The Competitive and Cultural Appeal}

The quest for $\pi$ is also a \textbf{global challenge}—a battle of \textbf{mathematicians, programmers, and supercomputers}.  

Every new world record invites someone to \textbf{break it}. It’s a \textbf{symbol of mathematical curiosity} and human achievement.  

And perhaps more importantly, $\pi$ is one of the few mathematical constants that \textbf{captivates the public}. It has been studied for over \textbf{4,000 years}, and yet, it remains \textbf{mysterious and infinite}.

As long as these mysteries remain, the \textbf{quest for $\pi$ will never end}.

With the rise of \textbf{quantum computing} it’s possible that we may one day compute $\pi$ at speeds unimaginable today. Even AI is starting to play a role in $\pi$ computations. AI models might analyze massive datasets of $\pi$'s digits to search for hidden structures or statistical anomalies.


That brings us to the end of this video. 
So, the next time you see π written as 3.14159, just remember—behind those digits lies a story of thousands of years of discovery, one that continues to unfold even today.

If you enjoyed exploring the fascinating world of π, make sure to like, subscribe, and let me know in the comments—what do you think the future of π computation will look like?

Thank you for watching, and Happy π Day!

\end{document}
